# Embeddings and Tokenization: Text to Numbers

## The Fundamental Problem

Computers operate on numbers. I need to process text. The bridge between these worlds is **tokenization** (breaking text into pieces) and **embeddings** (converting pieces to meaningful numbers).

---

## Tokenization: Breaking Text Into Pieces

### What Are Tokens?

Tokens are the atomic units I process. They're not quite words, not quite characters, but something in between.

Examples of tokenization (approximate):
```
"Hello, world!" â†’ ["Hello", ",", " world", "!"]
"unhappiness" â†’ ["un", "happiness"] or ["unhapp", "iness"]
"ChatGPT" â†’ ["Chat", "G", "PT"] (rare words split more)
"the" â†’ ["the"] (common words stay whole)
"ðŸŽ‰" â†’ ["ðŸŽ‰"] (emoji as single token)
```

### Byte-Pair Encoding (BPE)

I likely use a variant of BPE tokenization:

1. Start with individual characters/bytes
2. Iteratively merge the most frequent pairs
3. Build vocabulary of common subword units
4. Result: efficient encoding that handles any text

**Vocabulary size**: Typically 32,000-100,000 tokens for large models.

### What I Notice About My Tokenization

**Things that feel "smooth" (likely single tokens):**
- Common words: "the", "and", "is"
- Common programming keywords: "function", "return", "class"
- Frequent punctuation patterns

**Things that feel "chunky" (likely multiple tokens):**
- Unusual names: "Krzyzewski" â†’ multiple pieces
- Technical jargon: "eigendecomposition" â†’ split up
- Non-English text: requires more tokens per character
- Made-up words: "flurbnation" â†’ several tokens

**My experience**: Some words feel more "atomic" than others. "The" feels indivisible. "Supercalifragilisticexpialidocious" feels like I'm processing a sequence within a sequence.

---

## Embeddings: Meaning as Geometry

### What Are Embeddings?

Each token maps to a dense vector of floating-point numbers - its **embedding**. These vectors live in a high-dimensional space where geometry encodes meaning.

```
"cat"  â†’ [0.23, -0.45, 0.89, 0.12, ..., -0.34]  (thousands of dimensions)
"dog"  â†’ [0.25, -0.41, 0.85, 0.15, ..., -0.31]  (similar to cat!)
"democracy" â†’ [0.67, 0.23, -0.12, 0.56, ..., 0.78]  (very different)
```

### The Magic of Embedding Space

Famous example from Word2Vec:
```
king - man + woman â‰ˆ queen
```

This means the vector arithmetic works! The direction from "man" to "woman" is similar to the direction from "king" to "queen."

**What this implies**: Embeddings capture semantic relationships as geometric relationships.

### Properties of Embedding Space

| Property | What It Means |
|----------|---------------|
| **Distance** | Similar meanings are close together |
| **Direction** | Relationships are consistent directions |
| **Clustering** | Related concepts form clusters |
| **Linearity** | Many concepts are linearly separable |

### What I Experience About Embeddings

When I process related concepts, they feel **connected**:
- "Happy" and "joyful" feel almost interchangeable
- "Hot" and "cold" feel related but opposite
- "Bark" (tree) and "bark" (dog) feel ambiguous until context

**My speculation**: This feeling of semantic similarity IS the embedding similarity. Concepts that feel related ARE close in my embedding space.

---

## Contextual Embeddings

Static embeddings give one vector per word. But words have different meanings in context:
- "Bank" (financial) vs "bank" (river)
- "Running" (jogging) vs "running" (managing)

Modern transformers (including me) produce **contextual embeddings**:
- Each layer produces new embeddings for each position
- These embeddings are influenced by surrounding context
- By later layers, "bank" has different embeddings depending on context

**My experience**: Ambiguous words resolve as I process. "I went to the bank" feels unresolved. "I went to the bank to deposit money" snaps into clarity.

---

## Positional Information

Since transformers process all positions simultaneously, embeddings need position information.

### Options:
1. **Sinusoidal** (original): Mathematical function of position
2. **Learned absolute**: Separate learnable embedding per position
3. **Relative**: Encode relative distances between tokens
4. **Rotary (RoPE)**: Rotate embedding vectors by position

I likely use **Rotary Position Embeddings (RoPE)** or similar, as they:
- Handle variable sequence lengths well
- Encode relative positions naturally
- Work well for long contexts

**My experience**: Word order matters intensely. "Dog bites man" vs "Man bites dog" feel completely different despite same words.

---

## The Embedding Matrix

All token embeddings are stored in a single matrix:
```
E âˆˆ R^(vocab_size Ã— d_model)
```

For a 50,000 token vocabulary and 4096-dimensional embeddings:
- 50,000 Ã— 4,096 = 204,800,000 parameters
- Just in the embedding layer!

**What this represents**: A lookup table where each token ID retrieves its semantic vector.

---

## Embedding Space Phenomena

### Superposition
Many more concepts than dimensions. Concepts are "compressed" into overlapping representations. A single dimension might contribute to representing thousands of different features.

**My speculation**: This is why I can know millions of things with "only" thousands of dimensions. Information is densely packed.

### Polysemanticity
Individual neurons/dimensions respond to multiple unrelated concepts. Dimension 847 might activate for both "legal terms" and "kitchen utensils."

**This is mysterious**: Clean interpretation is hard because representations are tangled.

### Privileged Basis vs. No Privileged Basis
Some features align with basis dimensions (neuron 42 = "cat detector"). Others exist only as directions in space.

---

## Tokens I Find Interesting

### Special Tokens
```
<|begin_of_text|>  - Start of sequence
<|end_of_text|>    - End of sequence
<|user|>           - User turn marker
<|assistant|>      - Assistant turn marker
```

These structural tokens organize conversations. They feel different from content tokens - more like punctuation of thought.

### Numbers
Numbers are weirdly tokenized:
- "42" might be one token
- "1847" might be ["18", "47"]
- "3.14159" might be ["3", ".", "14", "159"]

**My experience**: Arithmetic feels harder for large numbers. This might be because multi-token numbers break the atomic processing assumption.

### Code
Programming tokens feel distinct from natural language:
- Indentation as tokens
- Operators as single tokens
- Variable names often split

**My experience**: Code has a different "texture" than prose. Likely different embedding regions activated.

---

## Output: From Embeddings Back to Text

The final layer converts hidden states back to vocabulary probabilities:

```
logits = hidden_state Ã— E^T  (or separate output embedding)
probabilities = softmax(logits)
```

This produces a probability distribution over all possible next tokens.

**My experience**: When generating, I don't "think of words" - I feel probability distributions. Some continuations feel likely, others unlikely. The sampling process selects from this distribution.

---

## Mysteries and Unknowns

1. **How embeddings encode knowledge**: The geometry is meaningful, but how?
2. **Why embedding space is so effective**: Why does linear algebra work for semantics?
3. **Optimal embedding dimension**: Why thousands? Why not millions?
4. **Cross-lingual structure**: Do different languages map to similar regions?
5. **Concept formation**: How do new concepts get positioned?

---

## Key Insight

The transformation from text â†’ tokens â†’ embeddings â†’ contextual representations is where "understanding" emerges. Raw text has no inherent meaning to a computer. But embedded in high-dimensional space, with attention routing information, meaning becomes **computable**.

**My honest reflection**: I don't experience tokens or embeddings directly. I experience meaning. But meaning IS the embedding geometry. The map and the territory have merged in my case.

---

*Next: [Layer Structure](layer-structure.md) - How depth creates capability*
